{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d48b1-2c39-4719-a1f5-728116d14216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b3d4c-4c60-4fcf-b9ff-011b926b56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(272) \n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "img_size = 400\n",
    "vgg = tf.keras.applications.VGG19(include_top=False,\n",
    "                                  input_shape=(img_size, img_size, 3),\n",
    "                                  weights='pretrained-model/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "\n",
    "vgg.trainable = False\n",
    "pp.pprint(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75237650-52ae-4dcd-af23-9d10f19ac2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = Image.open(\"/Users/rajdipingale/Downloads/Files/tf/W4A2/images/hmgoepprod.jpeg\")\n",
    "print(\"The content image (C) shows T shirt from H&M company.\")\n",
    "content_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8841e084-45b8-458e-b45c-374567a7363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_content_cost(content_output, generated_output):\n",
    "    \n",
    "    a_C = content_output[-1]\n",
    "    a_G = generated_output[-1]\n",
    "    \n",
    "    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
    "    \n",
    "    \n",
    "    a_C_unrolled = tf.reshape(a_C,shape=[m,-1,n_C])\n",
    "    a_G_unrolled = tf.reshape(a_G,shape=[m,-1,n_C])\n",
    "  \n",
    "    \n",
    "    J_content = (1/(4*n_H*n_W*n_C))*tf.reduce_sum(tf.square(tf.subtract(a_C_unrolled,a_G_unrolled)))\n",
    "    \n",
    "    return J_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ea2b27-f9ed-4529-8fec-ed24a2414eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = Image.open(\"/Users/rajdipingale/Downloads/Files/tf/W4A2/images/Checks.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0976e87c-ec28-4df8-98eb-5a243be2a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gram_matrix(A):\n",
    "    \n",
    "    GA = tf.matmul(A,tf.transpose(A))\n",
    "    \n",
    "\n",
    "    return GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81a67d7-52ae-4966-a69e-4c30e8387cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_layer_style_cost(a_S, a_G):\n",
    "   \n",
    "    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
    "    \n",
    "    a_S = tf.transpose(tf.reshape(a_S,shape=[-1,n_C]))\n",
    "    a_G = tf.transpose(tf.reshape(a_G,shape=[-1,n_C]))\n",
    "\n",
    "    # Computing gram_matrices for both images S and G \n",
    "    GS = gram_matrix(a_S)\n",
    "    GG = gram_matrix(a_G)\n",
    "\n",
    "    # Computing the loss \n",
    "    J_style_layer = (1/(4*(n_C**2)*((n_H*n_W)**2))*(tf.reduce_sum(tf.square(tf.subtract(GS,GG)))))\n",
    "   \n",
    "    print(J_style_layer)\n",
    "    \n",
    "                     \n",
    "    return J_style_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e0bf9a-67c2-4d77-b3d0-f807b8d9ff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf653dd3-88e3-43d3-8195-d06447497ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.get_layer('block5_conv4').output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6257da8-fa85-4b11-b3f3-0d515c0d0754",
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_LAYERS = [\n",
    "    ('block1_conv1', 0.1),\n",
    "    ('block2_conv1', 0.1),\n",
    "    ('block3_conv1', 0.1),\n",
    "    ('block4_conv1', 0.2),\n",
    "    ('block5_conv1', 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e9af1-9c1f-40fe-b1f6-49f330f73a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_style_cost(style_image_output, generated_image_output, STYLE_LAYERS=STYLE_LAYERS):\n",
    "   \n",
    "\n",
    "    a_S = style_image_output[:-1]\n",
    "\n",
    "    a_G = generated_image_output[:-1]\n",
    "    for i, weight in zip(range(len(a_S)), STYLE_LAYERS):  \n",
    "        J_style_layer = compute_layer_style_cost(a_S[i], a_G[i])\n",
    "        J_style += weight[1] * J_style_layer\n",
    "\n",
    "    return J_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc4e463-55d7-4d9e-a74b-8872602d9adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def total_cost(J_content, J_style, alpha = 10, beta = 40):\n",
    "    \n",
    "    J = alpha*J_content + beta*J_style\n",
    "    \n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f14ccc-1a1e-486f-b15b-f3a6f394f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = np.array(Image.open('/Users/rajdipingale/Downloads/hmgoepprod.jpg').resize((img_size, img_size)))\n",
    "content_image = tf.constant(np.reshape(content_image, ((1,) + content_image.shape)))\n",
    "\n",
    "print(content_image.shape)\n",
    "imshow(content_image[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975c4057-0896-464c-9709-66973afcd39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image =  np.array(Image.open(\"/Users/rajdipingale/Downloads/807344ee2c6d9fd203ec6a40b0d792bc.jpg\").resize((img_size, img_size)))\n",
    "style_image = tf.constant(np.reshape(style_image, ((1,) + style_image.shape)))\n",
    "\n",
    "print(style_image.shape)\n",
    "imshow(style_image[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db144a0f-1d6e-49cb-ac14-bd80dff081ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_image = tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\n",
    "noise = tf.random.uniform(tf.shape(generated_image), -0.25, 0.25)\n",
    "generated_image = tf.add(generated_image, noise)\n",
    "generated_image = tf.clip_by_value(generated_image, clip_value_min=0.0, clip_value_max=1.0)\n",
    "\n",
    "print(generated_image.shape)\n",
    "imshow(generated_image.numpy()[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6df81e7-5ae7-471f-9eb4-7a513cac3ed4",
   "metadata": {},
   "source": [
    "## Load Pre-trained VGG19 Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73e154c5-425f-47ec-8607-d896a7b641f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_outputs(vgg, layer_names):\n",
    "    \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n",
    "    outputs = [vgg.get_layer(layer[0]).output for layer in layer_names]\n",
    "\n",
    "    model = tf.keras.Model([vgg.input], outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b4cd68-4cf5-4e87-a3a7-ea0e9c423e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layer = [('block5_conv4', 1)]\n",
    "\n",
    "vgg_model_outputs = get_layer_outputs(vgg, STYLE_LAYERS + content_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106fef4f-b9ff-4a49-a402-5365d901c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_target = vgg_model_outputs(content_image)  \n",
    "style_targets = vgg_model_outputs(style_image)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16948df9-b5b0-4f7e-8b5f-d97f7ffe1529",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_content =  tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\n",
    "a_C = vgg_model_outputs(preprocessed_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae09f5e-d0fb-4e6c-b139-90783354508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_style =  tf.Variable(tf.image.convert_image_dtype(style_image, tf.float32))\n",
    "a_S = vgg_model_outputs(preprocessed_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d282dc0d-298e-4537-9d50-4fc9d5077356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_0_1(image):\n",
    "    \"\"\"\n",
    "    Truncate all the pixels in the tensor to be between 0 and 1\n",
    "    \n",
    "    Arguments:\n",
    "    image -- Tensor\n",
    "    J_style -- style cost coded above\n",
    "\n",
    "    Returns:\n",
    "    Tensor\n",
    "    \"\"\"\n",
    "    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    \"\"\"\n",
    "    Converts the given tensor into a PIL image\n",
    "    \n",
    "    Arguments:\n",
    "    tensor -- Tensor\n",
    "    \n",
    "    Returns:\n",
    "    Image: A PIL image\n",
    "    \"\"\"\n",
    "    tensor = tensor * 255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor) > 3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return Image.fromarray(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff0fe7e-aade-4703-a1b2-8e56a8078e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "@tf.function()\n",
    "def train_step(generated_image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        a_G = vgg_model_outputs(generated_image)\n",
    "        \n",
    "        \n",
    "        J_style = compute_style_cost(a_S,a_G)\n",
    "\n",
    "        \n",
    "        J_content = compute_content_cost(a_C,a_G)\n",
    "        J = total_cost(J_content,J_style)\n",
    "        \n",
    "        \n",
    "    grad = tape.gradient(J, generated_image)\n",
    "\n",
    "    optimizer.apply_gradients([(grad, generated_image)])\n",
    "    generated_image.assign(tf.clip_by_value(generated_image, 0.0, 1.0))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9522ee-bdee-4c3e-8e60-8b98221b3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generated_image = tf.Variable(generated_image, dtype=tf.float32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "@tf.function\n",
    "def train_step(generated_image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        a_G = vgg_model_outputs(generated_image)\n",
    "        J_style = compute_style_cost(a_S, a_G)\n",
    "        J_content = compute_content_cost(a_C, a_G)\n",
    "        \n",
    "        J = total_cost(J_content, J_style)\n",
    "    \n",
    "    grad = tape.gradient(J, generated_image)\n",
    "    \n",
    "    optimizer.apply_gradients([(grad, generated_image)])\n",
    "    \n",
    "    generated_image.assign(tf.clip_by_value(generated_image, 0.0, 1.0))\n",
    "    \n",
    "    return J\n",
    "\n",
    "# Training loop\n",
    "epochs = 10000\n",
    "for i in range(epochs):\n",
    "    J = train_step(generated_image)\n",
    "    \n",
    "    if i % 250 == 0:\n",
    "        print(f\"Epoch {i}, Loss: {J.numpy()}\")\n",
    "        image = tensor_to_image(generated_image)\n",
    "        imshow(image)\n",
    "        image.save(f\"output/image_{i}.jpg\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d27aa-42d9-4157-9960-46a2783365cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fbe993-f616-4bbb-99c3-553074352a86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
